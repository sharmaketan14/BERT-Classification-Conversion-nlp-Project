{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7g2PhcwQxbos",
   "metadata": {
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1687689454091,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "7g2PhcwQxbos"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/content/drive/MyDrive/Project\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ea906f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21451,
     "status": "ok",
     "timestamp": 1687689477036,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "f9ea906f",
    "outputId": "2e6b1bd0-9caf-435f-b4ee-5375d7659aa3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow_text\n",
      "  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (0.13.0)\n",
      "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.54.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.10)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.22.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (4.6.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.32.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.2.2)\n",
      "Installing collected packages: tensorflow_text\n",
      "Successfully installed tensorflow_text-2.12.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
      "Collecting sentencepiece (from sentence-transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.6.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=f2a180d30ac9ebbab0faead0e4f8ade8a65cc762e59fdc6ebee794cd90564b94\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, huggingface-hub, transformers, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.30.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_text\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f165adbd",
   "metadata": {
    "executionInfo": {
     "elapsed": 5046,
     "status": "ok",
     "timestamp": 1687689484462,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "f165adbd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "S15HXscO0XDF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23951,
     "status": "ok",
     "timestamp": 1687689509884,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "S15HXscO0XDF",
    "outputId": "e11f95cb-6637-4d2f-8d42-158eb0ac3f3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Xxug6DBfAd0V",
   "metadata": {
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1687689512245,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "Xxug6DBfAd0V"
   },
   "outputs": [],
   "source": [
    "def classify_sentence(sentence, model):\n",
    "    sentence = np.array([[sentence]])\n",
    "    print(sentence)\n",
    "    y_predict = model.predict(sentence)\n",
    "    y_predict = y_predict.flatten()\n",
    "    y_predict = np.where(y_predict > 0.4, 1, 0)\n",
    "\n",
    "    return y_predict[0]\n",
    "\n",
    "def prepare_data():\n",
    "  import pandas as pd\n",
    "  final_dataset = pd.read_csv(\"/content/drive/MyDrive/Project/Processed_Data/fin.csv\")\n",
    "  X = final_dataset[\"Comments\"]\n",
    "  y = final_dataset[\"Type\"]\n",
    "  final_dataset.head()\n",
    "\n",
    "  final_dataset.groupby('Type').describe()\n",
    "  final_dataset[\"Type\"].value_counts()\n",
    "\n",
    "    #df_roast = final_dataset[final_dataset[\"Type\"] == 0]\n",
    "  df_toast = final_dataset[final_dataset[\"Type\"] == 1]\n",
    "    #df_roast_downsampled = df_roast.sample(df_toast.shape[0])\n",
    "\n",
    "  sentences = []\n",
    "  for i in df_toast[\"Comments\"] :\n",
    "    sentences.append(i)\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def sentence_vec(sentences):\n",
    "  model_name = 'bert-base-nli-mean-tokens'\n",
    "  from sentence_transformers import SentenceTransformer\n",
    "\n",
    "  md = SentenceTransformer(model_name)\n",
    "\n",
    "  Sentence_vecs = md.encode(sentences)\n",
    "\n",
    "  return Sentence_vecs\n",
    "\n",
    "def get_convert_sentence(str1, Sentence_vecs, sentences):\n",
    "  model_name = 'bert-base-nli-mean-tokens'\n",
    "  from sentence_transformers import SentenceTransformer\n",
    "\n",
    "  md = SentenceTransformer(model_name)\n",
    "\n",
    "  sen = md.encode(str1)\n",
    "\n",
    "  from sklearn.metrics.pairwise import cosine_similarity\n",
    "  result = cosine_similarity([sen], Sentence_vecs[:])\n",
    "\n",
    "  m = 0\n",
    "  final_conversion = \"\"\n",
    "  for i in range(0, len(result[0])):\n",
    "    if(m<result[0][i]):\n",
    "      m = result[0][i]\n",
    "      final_conversion = sentences[i]\n",
    "\n",
    "  return final_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133d63bb",
   "metadata": {
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1687692038734,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "133d63bb"
   },
   "outputs": [],
   "source": [
    "def run_app(str1):\n",
    "  import pickle\n",
    "  #sentence = input(\"Enter the sentence: \")\n",
    "  model = load_model(\"./model\")\n",
    "  result = classify_sentence(str1, model)\n",
    "  final = \"\"\n",
    "  if(result == 0):\n",
    "    print(\"Roast Sentence\")\n",
    "    \n",
    "  else:\n",
    "    return \"Already Toast!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Ll973cmiPmQE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13825,
     "status": "ok",
     "timestamp": 1687689540415,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "Ll973cmiPmQE",
    "outputId": "f71d2d6d-62e9-4d5b-b0eb-e71fde044e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
      "Hit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:3 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Hit:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Hit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
      "Get:7 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Hit:9 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
      "Hit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:12 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,299 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,360 kB]\n",
      "Get:15 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [2,410 B]\n",
      "Fetched 5,022 kB in 3s (1,501 kB/s)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "13 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  ngrok\n",
      "0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded.\n",
      "Need to get 6,497 kB of archives.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.3.1 [6,497 kB]\n",
      "Fetched 6,497 kB in 2s (3,183 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package ngrok.\n",
      "(Reading database ... 123069 files and directories currently installed.)\n",
      "Preparing to unpack .../archives/ngrok_3.3.1_amd64.deb ...\n",
      "Unpacking ngrok (3.3.1) ...\n",
      "Setting up ngrok (3.3.1) ...\n"
     ]
    }
   ],
   "source": [
    "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Z37K_oyUtUpz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12799,
     "status": "ok",
     "timestamp": 1687689628781,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "Z37K_oyUtUpz",
    "outputId": "aa139220-7ac7-4b94-fa01-ff06e33e1825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting flask_ngrok\n",
      "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
      "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.2.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.27.1)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.3.6)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.1.2)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (8.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask_ngrok) (2.1.3)\n",
      "Installing collected packages: flask_ngrok\n",
      "Successfully installed flask_ngrok-0.0.25\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
      "Building wheels for collected packages: pyngrok\n",
      "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=bd04ba52398a2bee2be1f0bd1e70f29b26c158c58bd3738928d0c06a0c2c048e\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
      "Successfully built pyngrok\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-6.0.0\n",
      "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!pip install flask_ngrok\n",
    "!pip install pyngrok\n",
    "!ngrok config add-authtoken 2RZVrnpTreJEGpl3F0guVce60P5_4cmqLGEfNGvuBRLU1fpUt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2zrsRUv-DVdV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151515,
     "status": "ok",
     "timestamp": 1687691770562,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "2zrsRUv-DVdV",
    "outputId": "402ff519-9dcc-43ee-bebf-c49b4b4622ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Running on http://2e4d-35-204-221-90.ngrok-free.app\n",
      " * Traffic stats available on http://127.0.0.1:4040\n",
      "[['You should kill yourself                         ']]\n",
      "1/1 [==============================] - 1s 885ms/step\n",
      "Roast Sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [25/Jun/2023 11:14:58] \"POST /convert HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "from flask_ngrok import run_with_ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "run_with_ngrok(app)\n",
    "\n",
    "return_file = [\n",
    "    { 'sentence': \"Whats UP!\"}\n",
    "]\n",
    "\n",
    "vec_encodings = []\n",
    "@app.route('/', methods=['GET'])\n",
    "def get_incomes():\n",
    "  return jsonify(return_file)\n",
    "\n",
    "\n",
    "@app.route('/convert', methods=['POST'])\n",
    "def get_sentence():\n",
    "    str1 = request.form['sentence']\n",
    "    str1 = run_app(str1)\n",
    "    return_file[0]['sentence'] = str1\n",
    "    return jsonify(return_file)\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "BJ8ySB2t86Oc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 728,
     "status": "error",
     "timestamp": 1687692363627,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "BJ8ySB2t86Oc",
    "outputId": "c40f06c9-229e-4841-e9d7-51c6bd5f2471"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 15:59:36.744053: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-03-10 15:59:36.744098: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-03-10 15:59:36.744114: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-03-10 15:59:36.744200: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-10 15:59:36.744243: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Op type not registered 'CaseFoldUTF8' in binary running on Ketans-MacBook-Air.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrun_app\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYour fingers are like stick!!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mrun_app\u001b[0;34m(str1)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#sentence = input(\"Enter the sentence: \")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m classify_sentence(str1, model)\n\u001b[1;32m      6\u001b[0m final \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/saving/saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    231\u001b[0m         filepath,\n\u001b[1;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3749\u001b[0m, in \u001b[0;36mGraph.op_def_for_type\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   3746\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m]\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   3748\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m] \u001b[38;5;241m=\u001b[39m op_def_pb2\u001b[38;5;241m.\u001b[39mOpDef\u001b[38;5;241m.\u001b[39mFromString(\n\u001b[0;32m-> 3749\u001b[0m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_def_for_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3750\u001b[0m   )\n\u001b[1;32m   3751\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Op type not registered 'CaseFoldUTF8' in binary running on Ketans-MacBook-Air.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed."
     ]
    }
   ],
   "source": [
    "print(run_app(\"Your fingers are like stick!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "-AZ99PVq1tZF",
   "metadata": {
    "executionInfo": {
     "elapsed": 1020,
     "status": "ok",
     "timestamp": 1687691094277,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "-AZ99PVq1tZF"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"vec_encodings\", \"wb\") as f:\n",
    "  pickle.dump(vec_encodings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadKIBlNN5FF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5671,
     "status": "ok",
     "timestamp": 1687456044991,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "fadKIBlNN5FF",
    "outputId": "f7b7165b-d4e3-41b8-d663-21ae187f79c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting flask_ngrok\n",
      "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
      "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.2.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.27.1)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.3.0)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.1.2)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (8.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask_ngrok) (2.1.2)\n",
      "Installing collected packages: flask_ngrok\n",
      "Successfully installed flask_ngrok-0.0.25\n"
     ]
    }
   ],
   "source": [
    "!pip install flask_ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "igvAkhpC-dUx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28334,
     "status": "ok",
     "timestamp": 1687435568687,
     "user": {
      "displayName": "KETAN SHARMA 21BAI10361",
      "userId": "09105080905342425431"
     },
     "user_tz": -330
    },
    "id": "igvAkhpC-dUx",
    "outputId": "81c9b6cb-b05e-488c-a93c-282cc7181308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-22 12:05:41.873549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[31m * Ignoring a call to 'app.run()' that would block the current 'flask' CLI command.\n",
      "   Only call 'app.run()' in an 'if __name__ == \"__main__\"' guard.\u001b[0m\n",
      " * Serving Flask app 'drive/MyDrive/Project/App.py'\n",
      " * Debug mode: off\n",
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!flask --app drive/MyDrive/Project/App.py run\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
